{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecc629b-b0c2-4494-804e-8798777fea04",
   "metadata": {},
   "source": [
    "**Zero-Shot Prompting**\n",
    "\n",
    "\t• What it is: You give the model only the instructions, context, and question. No examples.\n",
    "\t• When to use:\n",
    "\t• When you don’t have sample Q&A pairs.\n",
    "\t• For general-purpose answering.\n",
    "\t• When you want the model to reason freely but still stay grounded in the context.\n",
    "\t• Trade-off: Faster and simpler, but sometimes answers may not follow the exact style or format you want.\n",
    "\n",
    "Example:\n",
    "\n",
    "Context about Transformers → Ask: “How do they handle long-range dependencies?”\n",
    "\n",
    "The model figures out the answer directly from context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ab975d-e1ad-4f87-b92b-0d854119149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U llama-index llama-index-llms-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e24fd0-a876-4d81-b3cc-93ebc0a33a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ZERO-SHOT ---\n",
    "import os\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key\"\n",
    "\n",
    "def run_llm_zeroshot(context: str, query: str) -> str:\n",
    "    llm = Groq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "\n",
    "    template_str = (\n",
    "        \"You are an expert AI assistant.\\n\"\n",
    "        \"Use ONLY use the provided context to answer the user's question. \"\n",
    "        \"If the context is insufficient or does not mention the answer, reply exactly: \"\n",
    "        \"'Not enough information.'\\n\\n\"\n",
    "        \"Context:\\n{context_str}\\n\\n\"\n",
    "        \"User Question: {query_str}\\n\\n\"\n",
    "        \"Answering Rules:\\n\"\n",
    "        \"1) Be concise and precise (3–6 sentences, unless the question requires more).\\n\"\n",
    "        \"2) Use bullet points for lists.\\n\"\n",
    "        \"3) At the end, include a 'Sources:' section with short snippets or filenames from the context you used.\\n\\n\"\n",
    "        \"Final Answer:\"\n",
    "    )\n",
    "    prompt = PromptTemplate(template_str).format(context_str=context, query_str=query)\n",
    "    response = llm.complete(prompt=prompt)\n",
    "    output = response.text\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316944a3-8525-4ca4-a7ea-a443fea8c086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers handle long-range dependencies through their self-attention mechanism. This mechanism enables each token in a sequence to attend to all other tokens, regardless of their position. As a result, Transformers can capture dependencies between tokens that are far apart in the sequence without relying on recurrence. The self-attention mechanism is a key component of the Transformer architecture, allowing it to effectively model complex relationships between tokens. This approach helps Transformers to better understand the context and meaning of the input sequence.\n",
      "\n",
      "Sources: Context snippet on Transformers and self-attention mechanism\n"
     ]
    }
   ],
   "source": [
    "# Zero-Shot: No examples, just context + query\n",
    "context_text = (\n",
    "    \"Transformers use a self-attention mechanism that allows each token \"\n",
    "    \"to attend to all other tokens in the sequence. This helps capture \"\n",
    "    \"long-range dependencies without recurrence.\"\n",
    ")\n",
    "\n",
    "query_text = \"How do Transformers handle long-range dependencies?\"\n",
    "\n",
    "ans0 = run_llm_zeroshot(context=context_text, query=query_text)\n",
    "\n",
    "print(ans0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
