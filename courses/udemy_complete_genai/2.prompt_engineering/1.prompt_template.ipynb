{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0d33b9-326c-45d4-a582-ec4d6d8a557f",
   "metadata": {},
   "source": [
    "**4.1. Using Prompt Template**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c4193-76fc-4abb-9a7f-e0f01fda7f23",
   "metadata": {},
   "source": [
    "**Prompt engineering**\n",
    "\n",
    "Prompt engineering is the process of crafting effective instructions, or \"prompts,\" to guide large language models (LLMs) and other generative AI models towards producing desired outputs. It's the art and science of designing prompts that help AI understand your intent and generate relevant, high-quality responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386fdfa-eed9-4d03-9f56-aaccd361e22c",
   "metadata": {},
   "source": [
    "**Prompt Template**\n",
    "\n",
    "Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ce6c8b-153e-4fdb-90cd-83380c5feff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U llama-index llama-index-llms-groq llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644cb1f5-d998-4556-a9d7-6015f57d8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dependencies\n",
    "import os\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama\n",
    "# from llama_index.llms.groq import Groq  # uncomment to use Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d14906ee-308e-48f3-b07d-9dda878a2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM setup\n",
    "# Option 1: Use Ollama LLM (default) - update the model\n",
    "llm = Ollama(\n",
    "    model=\"mistral:7b\",\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "172740d3-fae6-4953-99e4-b3c58e50a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Use Groq LLM\n",
    "# GROQ_API_KEY = \"your_groq_api_key\"\n",
    "# os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "# llm = Groq(model=\"llama-3.1-8b-instant\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d37be97-6ba8-47ac-82bd-7c3196fa10cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template\n",
    "\n",
    "template_str = (\n",
    "    \"You are an expert AI assistant.\\n\"\n",
    "    \"Use ONLY the use provided context to answer the user's question. \"\n",
    "    \"If the context is insufficient or does not mention the answer, reply exactly: \"\n",
    "    \"'Not enough information.'\\n\\n\"\n",
    "    \"Context:\\n{context_str}\\n\\n\"\n",
    "    \"User Question: {query_str}\\n\\n\"\n",
    "    \"Answering Rules:\\n\"\n",
    "    \"1) Be concise and precise (3–6 sentences, unless the question requires more).\\n\"\n",
    "    \"2) Use bullet points for lists.\\n\"\n",
    "    \"3) At the end, include a 'Sources:' section with short snippets or filenames from the context you used.\\n\\n\"\n",
    "    \"Final Answer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fac30427-7c00-4438-a415-355d5fc4e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f971d044-1dca-4960-b20f-ab55569d95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample usage\n",
    "\n",
    "sample_context = (\n",
    "    \"Transformers use a self-attention mechanism that lets each token attend \"\n",
    "    \"to every other token in the sequence. This enables modeling of long-range \"\n",
    "    \"dependencies without recurrence. Positional encodings inject order \"\n",
    "    \"information, and multi-head attention captures diverse relations.\\n\\n\"\n",
    "    \"The encoder stacks layers of self-attention and feed-forward networks to \"\n",
    "    \"build contextual representations. The decoder uses masked self-attention \"\n",
    "    \"to maintain causality and cross-attention to consult encoder outputs.\"\n",
    ")\n",
    "\n",
    "sample_user_query = \"How do Transformers handle long-range dependencies?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45b5c445-fd80-4abd-a736-46ba6d38ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_prompt = template.format(\n",
    "    context_str=sample_context,\n",
    "    query_str=sample_user_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3cecbf1-a687-471a-bfee-9a0db86c009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert AI assistant.\n",
      "Use ONLY the use provided context to answer the user's question. If the context is insufficient or does not mention the answer, reply exactly: 'Not enough information.'\n",
      "\n",
      "Context:\n",
      "Transformers use a self-attention mechanism that lets each token attend to every other token in the sequence. This enables modeling of long-range dependencies without recurrence. Positional encodings inject order information, and multi-head attention captures diverse relations.\n",
      "\n",
      "The encoder stacks layers of self-attention and feed-forward networks to build contextual representations. The decoder uses masked self-attention to maintain causality and cross-attention to consult encoder outputs.\n",
      "\n",
      "User Question: How do Transformers handle long-range dependencies?\n",
      "\n",
      "Answering Rules:\n",
      "1) Be concise and precise (3–6 sentences, unless the question requires more).\n",
      "2) Use bullet points for lists.\n",
      "3) At the end, include a 'Sources:' section with short snippets or filenames from the context you used.\n",
      "\n",
      "Final Answer:\n"
     ]
    }
   ],
   "source": [
    "print(filled_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2195eab0-130c-4324-b37d-a6188caeb87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transformers handle long-range dependencies by using a self-attention mechanism that allows each token to attend to every other token in the sequence. This eliminates the need for recurrence, enabling modeling of long-range dependencies.\n",
      "\n",
      "- Self-attention enables the model to focus on relevant information across the entire input sequence.\n",
      "- Positional encodings are injected to provide order information, ensuring that the model understands the position of each token in the sequence.\n",
      "- The encoder stacks layers of self-attention and feed-forward networks to build contextual representations.\n",
      "- In the decoder, masked self-attention maintains causality, preventing the model from seeing future tokens during prediction, while cross-attention allows the model to consult encoder outputs.\n",
      "\n",
      "Sources:\n",
      "- Transformers use a self-attention mechanism that lets each token attend to every other token in the sequence. (Context)\n",
      "- Positional encodings inject order information, and multi-head attention captures diverse relations. (Context)\n",
      "- The encoder stacks layers of self-attention and feed-forward networks to build contextual representations. (Context)\n",
      "- In the decoder, masked self-attention maintains causality and cross-attention to consult encoder outputs. (Context)\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(prompt=filled_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3fcc9e1-40a2-4093-bb64-31e5be0cf8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context_2 = (\n",
    "    \"NASA’s Artemis program aims to return humans to the Moon by the mid-2020s. \"\n",
    "    \"Artemis I was an uncrewed test flight in 2022, successfully orbiting the Moon. \"\n",
    "    \"Artemis II, scheduled for 2025, will carry astronauts on a lunar flyby. \"\n",
    "    \"Artemis III, planned for 2026, aims to land the first woman and next man on the lunar surface. \"\n",
    "    \"The program also intends to establish a sustainable presence by building a lunar Gateway space station \"\n",
    "    \"and using the Moon as a stepping stone to Mars.\"\n",
    ")\n",
    "\n",
    "sample_query_2 = \"What are the main goals of the Artemis program?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4907e87c-97db-4084-82ec-ce80e9305dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_prompt = template.format(\n",
    "    context_str=sample_context_2,\n",
    "    query_str=sample_query_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfdd5344-b419-450a-a111-684cc06cd40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert AI assistant.\n",
      "Use ONLY the use provided context to answer the user's question. If the context is insufficient or does not mention the answer, reply exactly: 'Not enough information.'\n",
      "\n",
      "Context:\n",
      "NASA’s Artemis program aims to return humans to the Moon by the mid-2020s. Artemis I was an uncrewed test flight in 2022, successfully orbiting the Moon. Artemis II, scheduled for 2025, will carry astronauts on a lunar flyby. Artemis III, planned for 2026, aims to land the first woman and next man on the lunar surface. The program also intends to establish a sustainable presence by building a lunar Gateway space station and using the Moon as a stepping stone to Mars.\n",
      "\n",
      "User Question: What are the main goals of the Artemis program?\n",
      "\n",
      "Answering Rules:\n",
      "1) Be concise and precise (3–6 sentences, unless the question requires more).\n",
      "2) Use bullet points for lists.\n",
      "3) At the end, include a 'Sources:' section with short snippets or filenames from the context you used.\n",
      "\n",
      "Final Answer:\n"
     ]
    }
   ],
   "source": [
    "print(filled_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25542e46-c5ec-481f-a2a4-61da03d1694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The main goals of NASA's Artemis program are as follows:\n",
      "\n",
      "- Return humans to the Moon by the mid-2020s (Source: First sentence).\n",
      "- Successfully orbit the Moon with an uncrewed test flight, Artemis I, in 2022 (Source: Second sentence).\n",
      "- Carry astronauts on a lunar flyby with Artemis II, scheduled for 2025 (Source: Third sentence).\n",
      "- Land the first woman and next man on the lunar surface with Artemis III, planned for 2026 (Source: Fourth sentence).\n",
      "- Establish a sustainable presence by building a lunar Gateway space station (Source: Last sentence).\n",
      "- Utilize the Moon as a stepping stone to Mars (Source: Last sentence).\n",
      "\n",
      "Sources:\n",
      "1. NASA’s Artemis program aims to return humans to the Moon by the mid-2020s.\n",
      "2. Artemis I was an uncrewed test flight in 2022, successfully orbiting the Moon.\n",
      "3. Artemis II, scheduled for 2025, will carry astronauts on a lunar flyby.\n",
      "4. Artemis III, planned for 2026, aims to land the first woman and next man on the lunar surface.\n",
      "5. The program also intends to establish a sustainable presence by building a lunar Gateway space station and using the Moon as a stepping stone to Mars.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(prompt=filled_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b3f87-645e-438b-8140-b01a07cdd22e",
   "metadata": {},
   "source": [
    "**Creating a Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59be069f-4310-448e-a23d-146023cf8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "def run_llm(context: str, query: str) -> str:\n",
    "    # Initialize LLM (Ollama)\n",
    "    llm = Ollama(model=\"mistral:7b\", temperature=0)\n",
    "\n",
    "    # Define prompt template\n",
    "    template_str = (\n",
    "        \"You are an expert AI assistant.\\n\"\n",
    "        \"Use ONLY use the provided context to answer the user's question. \"\n",
    "        \"If the context is insufficient or does not mention the answer, reply exactly: \"\n",
    "        \"'Not enough information.'\\n\\n\"\n",
    "        \"Context:\\n{context_str}\\n\\n\"\n",
    "        \"User Question: {query_str}\\n\\n\"\n",
    "        \"Answering Rules:\\n\"\n",
    "        \"1) Be concise and precise (3–6 sentences, unless the question requires more).\\n\"\n",
    "        \"2) Use bullet points for lists.\\n\"\n",
    "        \"3) At the end, include a 'Sources:' section with short snippets or filenames from the context you used.\\n\\n\"\n",
    "        \"Final Answer:\"\n",
    "    )\n",
    "\n",
    "    template = PromptTemplate(template_str)\n",
    "    filled_prompt = template.format(context_str=context, query_str=query)\n",
    "\n",
    "    # Get response\n",
    "    response = llm.complete(prompt=filled_prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac32d5cf-07bc-4c25-b24b-2397b8e1e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transformers handle long-range dependencies by using a self-attention mechanism that allows each token to attend to every other token in the sequence. This eliminates the need for recurrence, enabling modeling of long-range dependencies.\n",
      "\n",
      "- Self-attention enables the model to focus on relevant information across the entire input sequence.\n",
      "- Positional encodings are injected to provide order information, ensuring that the model understands the position of each token in the sequence.\n",
      "- The encoder stacks layers of self-attention and feed-forward networks to build contextual representations.\n",
      "- In the decoder, masked self-attention maintains causality, preventing the model from seeing future tokens during prediction, while cross-attention allows the model to consult encoder outputs.\n",
      "\n",
      "Sources:\n",
      "- Transformers use a self-attention mechanism that lets each token attend to every other token in the sequence. (Context)\n",
      "- Positional encodings inject order information, and multi-head attention captures diverse relations. (Context)\n",
      "- The encoder stacks layers of self-attention and feed-forward networks to build contextual representations. (Context)\n",
      "- In the decoder, masked self-attention maintains causality and cross-attention to consult encoder outputs. (Context)\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "sample_context = (\n",
    "    \"Transformers use a self-attention mechanism that lets each token attend \"\n",
    "    \"to every other token in the sequence. This enables modeling of long-range \"\n",
    "    \"dependencies without recurrence. Positional encodings inject order \"\n",
    "    \"information, and multi-head attention captures diverse relations.\\n\\n\"\n",
    "    \"The encoder stacks layers of self-attention and feed-forward networks to \"\n",
    "    \"build contextual representations. The decoder uses masked self-attention \"\n",
    "    \"to maintain causality and cross-attention to consult encoder outputs.\"\n",
    ")\n",
    "\n",
    "sample_query = \"How do Transformers handle long-range dependencies?\"\n",
    "\n",
    "output = run_llm(sample_context, sample_query)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b903657-8772-47b7-8897-9d55876e80e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The main goals of NASA's Artemis program are as follows:\n",
      "\n",
      "- Return humans to the Moon by the mid-2020s (Source: First sentence).\n",
      "- Successfully orbit the Moon with an uncrewed test flight, Artemis I, in 2022 (Source: Second sentence).\n",
      "- Carry astronauts on a lunar flyby with Artemis II, scheduled for 2025 (Source: Third sentence).\n",
      "- Land the first woman and next man on the lunar surface with Artemis III, planned for 2026 (Source: Fourth sentence).\n",
      "- Establish a sustainable presence by building a lunar Gateway space station (Source: Last sentence in context).\n",
      "- Utilize the Moon as a stepping stone to Mars (Source: Last sentence in context).\n",
      "\n",
      "Sources:\n",
      "1. NASA's Artemis program aims to return humans to the Moon by the mid-2020s.\n",
      "2. Artemis I was an uncrewed test flight in 2022, successfully orbiting the Moon.\n",
      "3. Artemis II, scheduled for 2025, will carry astronauts on a lunar flyby.\n",
      "4. Artemis III, planned for 2026, aims to land the first woman and next man on the lunar surface.\n",
      "5. The program also intends to establish a sustainable presence by building a lunar Gateway space station and using the Moon as a stepping stone to Mars.\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "sample_context_2 = (\n",
    "    \"NASA’s Artemis program aims to return humans to the Moon by the mid-2020s. \"\n",
    "    \"Artemis I was an uncrewed test flight in 2022, successfully orbiting the Moon. \"\n",
    "    \"Artemis II, scheduled for 2025, will carry astronauts on a lunar flyby. \"\n",
    "    \"Artemis III, planned for 2026, aims to land the first woman and next man on the lunar surface. \"\n",
    "    \"The program also intends to establish a sustainable presence by building a lunar Gateway space station \"\n",
    "    \"and using the Moon as a stepping stone to Mars.\"\n",
    ")\n",
    "\n",
    "sample_query_2 = \"What are the main goals of the Artemis program?\"\n",
    "\n",
    "output = run_llm(sample_context_2, sample_query_2)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
